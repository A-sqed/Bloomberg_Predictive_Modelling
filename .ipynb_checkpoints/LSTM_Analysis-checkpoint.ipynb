{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis to determine feat importance from 1 to 30 days \n",
    "Roll analysis out for each time period to check importance over time \n",
    "\n",
    "Library \n",
    "- https://medium.com/@hsahu/stock-prediction-with-xgboost-a-technical-indicators-approach-5f7e5940e9e3\n",
    "- https://hub.packtpub.com/cross-validation-strategies-for-time-series-forecasting-tutorial/\n",
    "- http://www.zhengwenjie.net/tscv/\n",
    "- https://github.com/TannerGilbert/Tutorials/tree/master/A%20guide%20to%20Ensemble%C2%A0Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    " - Momentum Features Appear to provide the highest vale for short predictions\n",
    " - Macro features become more impportant as you approach 30 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data...\n",
      "\n",
      "Adding 10 Day Ahead\n",
      "Splitting Test and Training Data...\n",
      "Encoding target training and test data...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import sys, time, datetime, SessionState\n",
    "import streamlit as st\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objs as go\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score, KFold, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import plot_roc_curve, plot_precision_recall_curve, roc_auc_score, mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures, scale, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import metrics, linear_model\n",
    "from sklearn import preprocessing\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DEBUG = True\n",
    "\n",
    "# Forecast timeperiod\n",
    "days_ahead = [10]\n",
    "\n",
    "# Input file date ranges\n",
    "date_start = datetime.date(2012, 8, 1)\n",
    "date_end = datetime.date(2020, 7, 30)\n",
    "\n",
    "# Read File\n",
    "file_buffer = \"./Economic_data_clean_20200801.xlsx\"\n",
    "session_state = pd.read_excel(file_buffer)\n",
    "\n",
    "# Set time period for analysis \n",
    "session_state['Dates'] = pd.to_datetime(session_state['Dates']).dt.date\n",
    "session_state= session_state[(session_state['Dates'] >= date_start) & \n",
    "                         (session_state['Dates'] <= date_end)]\n",
    "csv_data = session_state.copy()\n",
    "session_state = None\n",
    "\n",
    "# Split parameters\n",
    "split_perc = 0.20\n",
    "random_flag = False\n",
    "\n",
    "# Model Tuning \n",
    "_n_estimators = 30\n",
    "_learning_rate = 0.50\n",
    "_random = 1\n",
    "\n",
    "#..........................................................................\n",
    "#                           Pre-Processing            \n",
    "#..........................................................................\n",
    "\n",
    "if(DEBUG): print(\"Preprocessing data...\\n\")\n",
    "\n",
    "csv_data['EARN_DOWN'] = csv_data['EARN_DOWN'].astype(np.float16)\n",
    "\n",
    "csv_data['EARN_UP'] = csv_data['EARN_DOWN'].astype(np.float16)\n",
    "\n",
    "csv_data['CDX_HY_momentum'] = \\\n",
    "                    csv_data['CDX_HY'] .rolling(window=10).mean() -  \\\n",
    "                    csv_data['CDX_HY'] .rolling(window=30).mean() /  \\\n",
    "                    csv_data['CDX_HY'] .rolling(window=30).mean()\n",
    "\n",
    "csv_data['CDX_IG_momentum'] = \\\n",
    "                    csv_data['CDX_IG'] .rolling(window=10).mean() -  \\\n",
    "                    csv_data['CDX_IG'] .rolling(window=30).mean() /  \\\n",
    "                    csv_data['CDX_IG'] .rolling(window=30).mean()\n",
    "\n",
    "csv_data['INDEX_IG_momentum'] = \\\n",
    "                    csv_data['LUACTRUU_Index_OAS'] .rolling(window=10).mean() -  \\\n",
    "                    csv_data['LUACTRUU_Index_OAS'] .rolling(window=30).mean() /  \\\n",
    "                    csv_data['LUACTRUU_Index_OAS'] .rolling(window=30).mean()\n",
    "\n",
    "csv_data['INDEX_HY_momentum'] = \\\n",
    "                    csv_data['LF98TRUU_Index_OAS'] .rolling(window=10).mean() -  \\\n",
    "                    csv_data['LF98TRUU_Index_OAS'] .rolling(window=30).mean() /  \\\n",
    "                    csv_data['LF98TRUU_Index_OAS'] .rolling(window=30).mean()\n",
    "\n",
    "csv_data['GOLD_momentum'] =  \\\n",
    "                    csv_data['GOLD'] .rolling(window=10).mean() -  \\\n",
    "                    csv_data['GOLD'] .rolling(window=30).mean() /  \\\n",
    "                    csv_data['GOLD'] .rolling(window=30).mean()\n",
    "\n",
    "\n",
    "# For Each look ahead period \n",
    "for dh in days_ahead:\n",
    "    # Create columns to track index changes over forecasting period \n",
    "    if(DEBUG): print(\"Adding {} Day Ahead\".format(dh))\n",
    "    \n",
    "    # Test if target is higher or lower in days ahead (dh)\n",
    "    IG_forecast_name = 'Index_IG_{}_Days_Ahead'.format(dh)\n",
    "    csv_data[IG_forecast_name] = csv_data['LUACTRUU_Index_OAS'].shift(dh)\n",
    "    \n",
    "    HY_forecast_name = 'Index_HY_{}_Days_Ahead'.format(dh)\n",
    "    csv_data[HY_forecast_name] = csv_data['LF98TRUU_Index_OAS'].shift(dh)\n",
    "\n",
    "# Hold orginal data set \n",
    "complete_data = csv_data.dropna().copy()\n",
    "csv_data = None\n",
    "\n",
    "# Remove Target data from features\n",
    "X = complete_data.drop([IG_forecast_name, HY_forecast_name, 'Dates'], axis=1)\n",
    "feature_cols = X.columns\n",
    "X = scale(X)\n",
    "\n",
    "Y_HY = complete_data[HY_forecast_name]\n",
    "Y_IG = complete_data[IG_forecast_name]\n",
    "\n",
    "if(DEBUG): print(\"Splitting Test and Training Data...\")\n",
    "X_HY_train, X_HY_test, Y_HY_train, Y_HY_test = \\\n",
    "   train_test_split(X, Y_HY, test_size=split_perc, shuffle=random_flag)\n",
    "\n",
    "X_IG_train, X_IG_test, Y_IG_train, Y_IG_test = \\\n",
    "   train_test_split(X, Y_IG, test_size=split_perc, shuffle=random_flag)\n",
    "\n",
    "# Encode Target\n",
    "if(DEBUG): print(\"Encoding target training and test data...\")\n",
    "lab_enc = preprocessing.LabelEncoder()\n",
    "Y_HY_encoded = lab_enc.fit_transform(Y_HY)\n",
    "Y_IG_encoded = lab_enc.fit_transform(Y_IG)\n",
    "\n",
    "Y_HY_train_encoded = lab_enc.fit_transform(Y_HY_train)\n",
    "Y_IG_train_encoded = lab_enc.fit_transform(Y_IG_train)\n",
    "\n",
    "Y_HY_test_encoded = lab_enc.fit_transform(Y_HY_test)\n",
    "Y_IG_test_encoded = lab_enc.fit_transform(Y_IG_test)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "model = XGBRegressor()\n",
    "model.fit(X_IG_train,Y_IG_train)\n",
    "\n",
    "# Predict\n",
    "y_2 = model.predict(X_IG_test)\n",
    "tss = TimeSeriesSplit(n_splits=5).split(X)\n",
    "scores = cross_val_score(model, X, Y_IG, cv=tss)\n",
    "print(\"Mean cross-validataion score: %.2f\" % scores.mean())\n",
    "print(\"Accuracy: %0.2f (+/- %0.3f)\" % (scores.mean(), scores.std()))\n",
    "def feature_importance():\n",
    "    #importances = regr_2.best_estimator_.feature_importances_\n",
    "    importances = model.feature_importances_\n",
    "    feats = {} \n",
    "    for feature, importance in zip(feature_cols, importances):\n",
    "        feats[feature] = importance \n",
    "\n",
    "    feats = sorted(feats.items(), key=lambda x: x[1],  reverse=False) \n",
    "    feats = dict(feats)\n",
    "\n",
    "    width = 1\n",
    "    keys = feats.keys()\n",
    "    values = feats.values()\n",
    "    plt.figure(figsize=(15, 10)) \n",
    "    pl.barh(range(len(feats)), values, width, align='center', color=\"blue\")\n",
    "    plt.yticks(range(len(feats)), [\"{}\".format(v) for v in feats.keys()])\n",
    "\n",
    "    pl.title(\"Variable Importance\")\n",
    "    pl.xlabel(\"Relative Importance\")\n",
    "feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
